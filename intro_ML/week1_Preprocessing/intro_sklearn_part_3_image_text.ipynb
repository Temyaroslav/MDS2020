{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3. Extract feature from texts and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8)\n",
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset from sklearn\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "print(digits['images'].shape)\n",
    "print(digits['data'].shape)\n",
    "print(digits['target'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n",
       "       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n",
       "       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n",
       "       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n",
       "       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n",
       "       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n",
       "       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# images contain 8x8 black'n'white pictures of hadwritten digits\n",
    "digits['images'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Explore the data\n",
    "\n",
    "Let's plot several pictures, split dataset on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 15 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a plot\n",
    "_, axes = plt.subplots(3, 5)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, image, label in zip(axes, digits['images'], digits['target']):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that each pixel is a feature. Note, that images were already rechaped into vectors for us in `digits['data']` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# perform train test split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(digits['data'], digits['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train the model\n",
    "\n",
    "We can now train the model. This is a classification task with 10 classes. \n",
    "\n",
    "The model, that we will use is called `KNeighborsClassifier` (kNN). It does not have trainable parameters and works in the following manner:\n",
    "* Remember the training data\n",
    "* When new point arrives \n",
    "    * find `K` nearest points in the training dataset (e.g. by euclidean distance)\n",
    "    * return the most freaquent class among these `K`.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Create and train the following pipeline:\n",
    "* Scale the input vectorized image \n",
    "* Classify by kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaling',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('clf',\n",
       "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=None, n_neighbors=5, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# define pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('clf', KNeighborsClassifier())\n",
    "])\n",
    "# fit\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evaluate on test dataset\n",
    "\n",
    "We will use accuracy - proportion of correct predictions, to measure the quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# predict on test set\n",
    "y_pred = pipe.predict(X_test)\n",
    "# compute accuracy\n",
    "score = accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional task**\n",
    "\n",
    "Implement accuracy youself usinf numpy only;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_accuracy(true, predicted):\n",
    "    # your code here\n",
    "    return np.where(predicted == true)[0].shape[0] / len(predicted)\n",
    "    \n",
    "# test that your function work the same as `accuracy_score` from sklearn\n",
    "my_accuracy(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Convert documents to vectors\n",
    "\n",
    "Note that out dataset now is a set of documents (or texts): $D = (d_1, \\dots, d_N)$\n",
    "\n",
    "We will assume that there is vocabulary of size $M$, which contains all possible words, from which our documents are composed. \n",
    "\n",
    "Of course, each documents has different number of words in it. \n",
    "\n",
    "Today, we will consider 2 options how to represent texts with the vectors (embeddings):\n",
    "1. Bag of words\n",
    "2. Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us firstly use the simplest example to undertand how both methods work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"This is my favourite movie\"\n",
    "d2 = \"Is this movie boring? Yes, it is!\"\n",
    "d3 = \"This is an exiting movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'my', 'favourite', 'movie'],\n",
       " ['is', 'this', 'movie', 'boring', 'yes', 'it', 'is'],\n",
       " ['this', 'is', 'an', 'exiting', 'movie']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "D = [re.sub('[.!?,]', '', d.lower()).split(' ') for d in [d1, d2, d3]]\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vocabulary for such dataset would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'my', 'exiting', 'favourite', 'this', 'boring', 'yes', 'an', 'it', 'is']\n"
     ]
    }
   ],
   "source": [
    "all_words = sum(D, [])\n",
    "V = list(set(all_words))\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1. Bag of words\n",
    "\n",
    "We can say, that text is charaterized by the vector of length $M$, which shows how many times each word from the vector is present in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now calculate bag of words for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(D), len(V)))\n",
    "\n",
    "for j, v in enumerate(V):\n",
    "    for i, d in enumerate(D):\n",
    "        X[i, j] = sum([1 for w in d if w == v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 1., 1., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 1., 1., 1., 0., 1., 2.],\n",
       "       [1., 0., 1., 0., 1., 0., 0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2. Tf-idf\n",
    "\n",
    "**Term Frequency times Inverce Document Frequency**\n",
    "\n",
    "A method to describe each document in the dataset with a vector of the same length. Takes into account, how often the word appears in the whole dataset.\n",
    "\n",
    "\n",
    "\n",
    "**Term frequency (tf)** - number of times a term occurs in a given document\n",
    "$$\n",
    "tf(t, d) = \\frac{\\# t \\text{ in } d}{len(d)}\n",
    "$$\n",
    "\n",
    "\n",
    "**Inverce document frequency (idf)** - measures informativeness of a term\n",
    "\n",
    "$$\n",
    "idf(t) = \\log \\frac{N}{(\\# d \\text{ with } t)} , N - \\text{ number of documents}\n",
    "$$\n",
    "\n",
    "If the word occures almost in all the documents (e.g. article, popular verb), then $idf$ will be very low.\n",
    "\n",
    "---\n",
    "Now we can covert each document onti the vector of size $M$:\n",
    "$$\n",
    "d \\rightarrow \\left(tf(t_1, d)\\cdot idf(t_1),\\,\\, \\dots, \\,\\, tf(t_M, d) \\cdot idf(t_M)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "Let us calculate it for our simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'my', 'favourite', 'movie'], ['is', 'this', 'movie', 'boring', 'yes', 'it', 'is'], ['this', 'is', 'an', 'exiting', 'movie']] \n",
      "\n",
      "['movie', 'my', 'exiting', 'favourite', 'this', 'boring', 'yes', 'an', 'it', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(D, '\\n')\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf\n",
    "tf = np.zeros((len(D), len(V)))\n",
    "\n",
    "for j, v in enumerate(V):\n",
    "    for i, d in enumerate(D):\n",
    "        tf[i, j] = sum([1 for w in d if w == v]) / len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf\n",
    "idf = np.zeros(len(V))\n",
    "\n",
    "for j, v in enumerate(V):\n",
    "    idf[j] = np.log(len(D) / sum([1 for d in D if v in d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.22, 0.  , 0.22, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.16, 0.  , 0.16, 0.  ],\n",
       "       [0.  , 0.  , 0.22, 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf*idf\n",
    "np.round(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Optional task**\n",
    "\n",
    "Below we will use `BoW` and `tf-idf` features to classify texts from the dataset with news articles. \n",
    "\n",
    "### BoW and Tf-Idf in Sklearn\n",
    "\n",
    "In practice, we can use `Trasfromers` from sklearn to get the same vector representation of texts as we implemented above. \n",
    "\n",
    "#### 1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "categories = ['alt.atheism', 'sci.space']\n",
    "train = pd.read_csv('train_news.csv', index_col=0)\n",
    "X_train, y_train = train.news, train.target\n",
    "\n",
    "\n",
    "test = pd.read_csv('test_news.csv', index_col=0)\n",
    "X_test, y_test = test.news, test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Explore the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0\n",
      ": \n",
      ": >> Please enlighten me.  How is omnipotence contradictory?\n",
      ": \n",
      ": >By definition, all that can occur in the universe is governed by the rules\n",
      ": >of nature. Thus god cannot break them. Anything that god does must be allowed\n",
      ": >in the rules somewhere. Therefore, omnipotence CANNOT exist! It contradicts\n",
      ": >the rules of nature.\n",
      ": \n",
      ": Obviously, an omnipotent god can change the rules.\n",
      "\n",
      "When you say, \"By definition\", what exactly is being defined;\n",
      "certainly not omnipotence. You seem to be saying that the \"rules of\n",
      "nature\" are pre-existant somehow, that they not only define nature but\n",
      "actually cause it. If that's what you mean I'd like to hear your\n",
      "further thoughts on the question.\n",
      "-------\n",
      "\n",
      "label: 1\n",
      "In <19APR199320262420@kelvin.jpl.nasa.gov> baalke@kelvin.jpl.nasa.gov \n",
      "\n",
      "Sorry I think I missed a bit of info on this Transition Experiment. What is it?\n",
      "\n",
      "Will this mean a loss of data or will the Magellan transmit data later on ??\n",
      "\n",
      "BTW: When will NASA cut off the connection with Magellan?? Not that I am\n",
      "looking forward to that day but I am just curious. I believe it had something\n",
      "to do with the funding from the goverment (or rather _NO_ funding :-)\n",
      "\n",
      "ok that's it for now. See you guys around,\n",
      "Jurriaan.\n",
      " \n",
      "-------\n",
      "\n",
      "label: 1\n",
      "\n",
      "Henry, I made the assumption that he who gets there firstest with the mostest\n",
      "wins. \n",
      "\n",
      "Ohhh, you want to put in FINE PRINT which says \"Thou shall do wonderous R&D\n",
      "rather than use off-the-shelf hardware\"? Sorry, didn't see that in my copy.\n",
      "Most of the Pournellesque proposals run along the lines of <some dollar\n",
      "amount> reward for <some simple goal>.  \n",
      "\n",
      "You go ahead and do your development, I'll buy off the shelf at higher cost (or\n",
      "even Russian; but I also assume that there'd be some \"Buy US\" provos in there)\n",
      "and be camped out in the Moon while you are launching and assembling little\n",
      "itty-bitty payloads in LEO with your laser or gas gun.  And working out the\n",
      "bugs of assembly & integration in LEO. \n",
      "\n",
      "Oh, hey, could I get a couple of CanadARMs tuned for the lunar environment?  I\n",
      "wanna do some teleoperated prospecting while I'm up there...\n",
      "\n",
      "\n",
      "\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('label:',y_train[i])\n",
    "    print(X_train[i])\n",
    "    print('-------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 BoW \n",
    "\n",
    "Our pipeline:\n",
    "* BoW vectorizer\n",
    "* kNN classifier\n",
    "\n",
    "We will use accuracy to evaluate model on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define traning pipeline\n",
    "bow = CountVectorizer(min_df=0.1, stop_words='english')\n",
    "\n",
    "pipe = # your code here\n",
    "\n",
    "# Fit on train data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test data (compute accuracy)\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Tf-Idf\n",
    "\n",
    "Let's repeat the same procedure, but for tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define traning pipeline\n",
    "tf_idf = TfidfVectorizer(sublinear_tf=True, min_df=0.1, stop_words='english')\n",
    "\n",
    "pipe = # your code here\n",
    "\n",
    "\n",
    "# Fit\n",
    "# your code here\n",
    "\n",
    "# Evaluate on test\n",
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
