<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Bayesian Statistics</title>
        <meta charset="UTF-8">
        <!-- main stylesheet -->
        <link rel="stylesheet" href="page_style.css">
        <!-- fonts -->
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <!-- The following library is used for math equations -->
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async
                src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
    </head>
    <body>
        <!-- Header -->
        <header class="page-header">
            <h1 class="page-margin page-jumbo">Bayesian Statistics</h1>
            <p class="page-xlarge">Introduction to Bayesian Inference</p>
        </header>

        <!-- First Grid -->
        <div class="page-top-padding page-row-padding">
            <div class="page-content">
                <div id="toc_container">
                    <p class="toc_title">Contents</p>
                    <ul class="toc_list">
                        <li><a href="#Motivation">Motivation</a>
                        <li><a href="#Inference">Bayes' Rule</a>
                        <li><a href="#Illustration">Illustration</a>
                    </ul>
                </div>
                <div>
                    <h1 class="page-p-header" id="Motivation">Motivation</h1>
                    <p>
                        Bayesian statistics is a theory in the field of statistics based on the Bayesian interpretation of probability where probability expresses 
                        a degree of belief in an event. The degree of belief may be based on prior knowledge about the event, such as the results of previous experiments, or on personal beliefs about the event. 
                        One of the major advantages is that Bayesian statistics provides us with the necessary toolbox to update our prior beliefs about random events as we observe these events. In other words,
                        it allows us to <i>tie in our prior beliefs and the new evidence in order to produce new posterior beliefs</i>.
                    </p>
                </div>
                <div>
                    <h1 class="page-p-header" id="Inference">Using Bayes' Rule for Inference</h1>
                    <p class="page-formula">
                        \(P(\theta|Y)= \frac{P(Y|\theta)P(\theta)}{P(Y)}\)
                    </p>
                    <ul>
                        <li>
                            <strong>\(P(\theta)\)</strong> is the <strong>prior</strong> probability distribution. In other words, this is our belief in the value of parameter
                            <strong>\(\theta\)</strong> without evidence <strong>\(Y\)</strong>.
                        </li>
                        <li> 
                            <strong>\(P(Y|\theta)\)</strong> is the <strong>likelihood</strong>. This is the probability of seeing the data that was generated by 
                            the conditional distribution using the parameter <strong>\(\theta\)</strong>.
                        </li>
                        <li>
                            <strong>\(P(Y)\)</strong> is the <strong>evidence</strong>. This is the probability of the data as determined 
                            by summing (or integrating) across all possible values of <strong>\(\theta\)</strong>,
                            weighted by how strongly we believe in those particular values of <strong>\(\theta\)</strong>.
                        </li>
                        <li>
                            <strong>\(P(\theta|Y)\)</strong> is the <strong>posterior</strong> probability distribution. 
                            This is the "updated" value of the parameter <strong>\(\theta\)</strong> once the evidence <strong>\(Y\)</strong> has been taken into account.
                            Over the course of observing or generating some data <strong>\(Y\)</strong>, Bayes' rule helps us to assess the probability of seeing data
                            <strong>\(Y\)</strong> given the parameter <strong>\(\theta\)</strong>.
                        </li>
                    </ul>
                    <p>
                        What is appealing about Bayesian Inference is the fact that we can use the posterior probability distribution as the new prior when we get new evidence.
                        Therefore, by repeatedly using the Bayes' Rule above we will be able to continually adjust our beliefs as we observe new data flow in.
                        Now let's see how does it work exactly on a simple example like coin tossing.
                    </p>
                </div>
            </div>
        </div>

        <!-- Second Grid -->
        <div class="page-row-padding page-grid-color">
            <div class="page-content">
                <div>
                    <h1 class="page-p-header" id="Illustration">Illustrating Bayesian Inference with Coin Tossing</h1>
                    <p>
                        In this experiment we are going to toss a coin multiple times and try to find out if the coin is fair or not.
                        Since we have no idea how fair the coin is, it makes sense to use a 
                        <a href="http://en.wikipedia.org/wiki/Uniform_distribution_%28continuous%29">uniform distribution</a> 
                        as the choice of our <strong>prior</strong> distribution.
                    </p>
                    <p>
                        To simulate our coin tossing results we are going to use SciPy's
                        <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bernoulli.html">Bernoulli</a> 
                        random variable implementation with the number of tossings equal to \(n=500\) and \(\theta=0.75\).
                        Since the coin is not fair, we are going to see more heads than tails and expect our Bayesian Inference
                        algorithm to adjust the <strong>posteior</strong> distribution accordingly. 
                    </p>
                    <p>

                        To update our posterior beliefs we are going to use a very convenient thing called
                        <i>
                            <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior</a>
                        </i>
                        , which says that under the 
                        <a href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial</a>
                        likelihood function <strong>\(P(Y|\theta)\)</strong> used in our experiment, our posterior distribution turns out to be a
                        <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a> of form:
                    </p>
                    <p class="page-formula">
                        \(P(\theta|\alpha,\beta)= \theta^{\alpha-1}(1 - \theta)^{\beta - 1}/B(\alpha, \beta)\)
                    </p>
                    <p>where \(B(\alpha, \beta)\) is a normalising constant and \(\alpha\) and \(\beta\) are some parameters.
                        You can find more details about conjugate priors and Beta distributions in 
                        <a href="https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af">this</a> beautiful article.
                    </p>
                    <p>
                        Now we have everything ready to proceed to simulation. Quite conveniently, uniform distribution is also the special case of Beta distribution
                        with the parameters \(\alpha=1, \beta=1\):
                    </p>
                    <img alt="Beta Distribution" class="page-pic" src="beta.png">
                    <p>
                        Therefore, we can make all the calculations for our experiment using SciPy's
                        <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html">Beta distribution</a> module.
                        Let's see what the results look like:
                    </p>
                    <img alt="Si,ulation Results" class="page-pic" src="sim.png">
                    <p>
                        From the charts we can see how we have started with the uniform distribution and hence no particular prior belief about the fairness of the coin
                        and kept updating our beliefs. After 500 tossings we are pretty much sure that the coin is unfair, so the resulting posterior probability distribution is
                        "centered" more around \(\theta=0.75\) with the lower standard deviation: 
                    </p>
                    <table>
                        <tr>
                          <th>\(n\) tosses</th>
                          <th># heads</th>
                          <th>\(E(X)\)</th>
                          <th>\(std(X)\)</th>
                        </tr>
                        <tr>
                          <td>0</td>
                          <td>0</td>
                          <td>0.5</td>
                          <td>0.288</td>
                        </tr>
                        <tr>
                          <td>2</td>
                          <td>2</td>
                          <td>0.75</td>
                          <td>0.193</td>
                        </tr>
                        <tr>
                          <td>10</td>
                          <td>9</td>
                          <td>0.83</td>
                          <td>0.103</td>
                        </tr>
                        <tr>
                          <td>50</td>
                          <td>38</td>
                          <td>0.75</td>
                          <td>0.059</td>
                        </tr>
                        <tr>
                          <td>100</td>
                          <td>74</td>
                          <td>0.735</td>
                          <td>0.043</td>
                        </tr>
                        <tr>
                          <td>500</td>
                          <td>367</td>
                          <td>0.733</td>
                          <td>0.019</td>
                        </tr>
                      </table>
                    <br>
                </div>
            </div>
        </div>
        <!-- Footer -->
        <footer class="page-footer"> 
            <p>
                Thanks for your attention!
            </p>
        </footer>
    </body>
</html>
